{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5491f5c3",
   "metadata": {},
   "source": [
    "There is a parent organization called \"ABC Group Inc\", which has two child companies named Tech Inc and MPTech.\n",
    "\n",
    "Both companies employee information is given in two separate text file as below. Please do the following activity for employee details.\n",
    "\n",
    "Tech Inc.txt\n",
    "\n",
    "1,Alok,Hyderabad\n",
    "\n",
    "2,Krish,Hongkong\n",
    "\n",
    "3,Jyoti,Mumbai\n",
    "\n",
    "4,Atul,Banglore\n",
    "\n",
    "5,Ishan,Gurgaon\n",
    "\n",
    "MPTech.txt\n",
    "\n",
    "6,John,Newyork\n",
    "\n",
    "7,alp2004,California\n",
    "\n",
    "8,tellme,Mumbai\n",
    "\n",
    "9,Gagan21,Pune\n",
    "\n",
    "10,Mukesh,Chennai\n",
    "\n",
    "1. Which command will you use to check all the available command line options on HDFS and How will you get the Help for individual command.\n",
    "\n",
    "2. Create a new Empty Directory named Employee using Command line. And also create an empty file named in it Techinc.txt\n",
    "\n",
    "3. Load both companies Employee data in Employee directory (How to override existing file in HDFS).\n",
    "\n",
    "4. Merge both the Employees data in a Single tile called MergedEmployee.txt, merged tiles should have new line character at the end of each file content.\n",
    "\n",
    "5. Upload merged file on HDFS and change the file permission on HDFS merged file,so that owner and group member can read and write, other user can read the file.\n",
    "\n",
    "6. Write a command to export the individual file as well as entire directory from HDFS to local file System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be5d8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create files using vi\n",
    "!echo -e \"1,Alok,Hyderabad\\n2,Krish,Hongkong\\n3,Jyoti,Mumbai\\n4,Atul,Banglore\\n5,Ishan,Gurgaon\" | hdfs dfs -put -f - Employee/Techinc.txt\n",
    "!echo -e \"6,John,Newyork\\n7,alp2004,California\\n8,tellme,Mumbai\\n9,Gagan21,Pune\\n10,Mukesh,Chennai\" | hdfs dfs -put -f - Employee/MPTech.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c6e8b",
   "metadata": {},
   "source": [
    "### Which command will you use to check all the available command line options on HDFS and How will you get the Help for individual command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adc99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733269c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set/satisfyStoragePolicy block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "sps                  run external storagepolicysatisfier\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "!hdfs --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8787f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-put [-f] [-p] [-l] [-d] <localsrc> ... <dst> :\n",
      "  Copy files from the local file system into fs. Copying fails if the file already\n",
      "  exists, unless the -f flag is given.\n",
      "  Flags:\n",
      "                                                                       \n",
      "  -p  Preserves access and modification times, ownership and the mode. \n",
      "  -f  Overwrites the destination if it already exists.                 \n",
      "  -l  Allow DataNode to lazily persist the file to disk. Forces        \n",
      "         replication factor of 1. This flag will result in reduced\n",
      "         durability. Use with care.\n",
      "                                                        \n",
      "  -d  Skip creation of temporary file(<dst>._COPYING_). \n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -help put"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4efbd3",
   "metadata": {},
   "source": [
    "### Create a new Empty Directory named Employee using Command line. And also create an empty file named in it Techinc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0921b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26166527",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -touchz Employee/Techinc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f600f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 itv000596 supergroup          0 2021-06-14 18:16 Employee/Techinc.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls Employee/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f2010",
   "metadata": {},
   "source": [
    "### Load both companies Employee data in Employee directory (How to override existing file in HDFS).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8a6eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `Employee/cca/Tech Inc.txt': File exists\n",
      "put: `Employee/cca/MPTech.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put -d ./cca/ Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a9a9dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 itv000596 supergroup          1 2021-06-14 18:26 Employee/-._COPYING_\n",
      "-rw-r--r--   3 itv000596 supergroup         85 2021-06-14 18:31 Employee/MPTech.txt\n",
      "-rw-r--r--   3 itv000596 supergroup          0 2021-06-14 18:16 Employee/Techinc.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7753739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 18:32:05,732 INFO fs.TrashPolicyDefault: Moved: 'hdfs://m01.itversity.com:9000/user/itv000596/Employee/cca' to trash at: hdfs://m01.itversity.com:9000/user/itv000596/.Trash/Current/user/itv000596/Employee/cca1623709925703\n"
     ]
    }
   ],
   "source": [
    "# remove a directory if its not empty\n",
    "# !hdfs dfs -rm -r -f Employee/cca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a04753",
   "metadata": {},
   "source": [
    "### Merge both the Employees data in a Single tile called MergedEmployee.txt, merged tiles should have new line character at the end of each file content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "312934bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-getmerge [-nl] [-skip-empty-file] <src> <localdst> :\n",
      "  Get all the files in the directories that match the source file pattern and\n",
      "  merge and sort them to only one file on local fs. <src> is kept.\n",
      "                                                                     \n",
      "  -nl               Add a newline character at the end of each file. \n",
      "  -skip-empty-file  Do not add new line character for empty file.    \n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -help getmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b10e7a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,John,Newyork\n",
      "7,alp2004,California\n",
      "8,tellme,Mumbai\n",
      "9,Gagan21,Pune\n",
      "10,Mukesh,Chennai\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Employee/MPTech.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1ce7940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,Alok,Hyderabad\n",
      "2,Krish,Hongkong\n",
      "3,Jyoti,Mumbai\n",
      "4,Atul,Banglore\n",
      "5,Ishan,Gurgaon\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Employee/Techinc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5470f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -getmerge -nl -skip-empty-file \"Employee/*.txt\" Employee/MergedEmployee.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c19f2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 itv000596 supergroup          1 2021-06-14 18:26 Employee/-._COPYING_\n",
      "-rw-r--r--   3 itv000596 supergroup         85 2021-06-14 18:31 Employee/MPTech.txt\n",
      "-rw-r--r--   3 itv000596 supergroup         81 2021-06-14 18:54 Employee/Techinc.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4459ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,John,Newyork\n",
      "7,alp2004,California\n",
      "8,tellme,Mumbai\n",
      "9,Gagan21,Pune\n",
      "10,Mukesh,Chennai\n",
      "\n",
      "1,Alok,Hyderabad\n",
      "2,Krish,Hongkong\n",
      "3,Jyoti,Mumbai\n",
      "4,Atul,Banglore\n",
      "5,Ishan,Gurgaon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat Employee/MergedEmployee.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec4a999",
   "metadata": {},
   "source": [
    "### Upload merged file on HDFS and change the file permission on HDFS merged file,so that owner and group member can read and write, other user can read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53515360",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put Employee/MergedEmployee.txt Employee/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a252f126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,John,Newyork\n",
      "7,alp2004,California\n",
      "8,tellme,Mumbai\n",
      "9,Gagan21,Pune\n",
      "10,Mukesh,Chennai\n",
      "\n",
      "1,Alok,Hyderabad\n",
      "2,Krish,Hongkong\n",
      "3,Jyoti,Mumbai\n",
      "4,Atul,Banglore\n",
      "5,Ishan,Gurgaon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Employee/MergedEmployee.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65d661ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 600(400+200) - read+write for owner, 60 (40+20) - read+write for group , 4 - read for user \n",
    "!hdfs dfs -chmod 664 Employee/MergedEmployee.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb0aa72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--   3 itv000596 supergroup        168 2021-06-14 19:03 Employee/MergedEmployee.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls Employee/MergedEmployee.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f805d7a",
   "metadata": {},
   "source": [
    "### Write a command to export the individual file as well as entire directory from HDFS to local file System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "43ac015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get \"Employee/*.txt\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdfa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
